---
title: "Project 1"
author: "Jaksonm Johnson jaj4229"
date: ""
output:
  html_document: default
  pdf_document: default
---

```{r global_options, include=FALSE}
library(knitr)
opts_chunk$set(fig.align="center", fig.height=5, message=FALSE, fig.width=8,tidy.opts=list(width.cutoff=60),tidy=TRUE)
```
Intro: The datasets I have chosen to work with are "drug_use_table" and "CrimeStatebyState_2_". The "drug_use_table" dataset is sourced from Center for Behavioral Health Statistics and Quality and Substance Abuse and Mental Health Services Administration, both of which collected data from every state regarding self-reported drug use from 2004-2014. This data was then complied into one comprehensive dataset. The "drug_use_table" contains 31 variables, 28 of which represent the number of people who self-reported the use of specific drug. The other 3 varibles in the dataset are "Year", "Total", and "None reported".The "CrimeStatebyState_2_" was acquired from the FBI's Uniform Crime Reporting Statistics website and contains estimated crime statistics for each state from 2004-2014. The dataset contains 13 variables, 10 of which represent the estimated crime rate of various crimes. The other 3 variables in the dataset are "Year", "State", and "Population". These datasets were interesting to me, because I thought it would be interesting to see how drug use is generally correlated with crime. The only association I expect to see is a general positive relationship between total drug use and total crime rates (drug use is not measured as a crime statistic).  
```{R}
library(readxl)
CrimeStatebyState_2_<-read_xlsx("C:/Users/jakso/Documents/CrimeStatebyState (2).xlsx")
drug_use_table<-read_xlsx("C:/Users/jakso/OneDrive/drug use table.xlsx")
library(tidyverse)
library(formatR)
library(cluster)
full_data_P1<- full_join(drug_use_table, CrimeStatebyState_2_)
full_data_P1 <- full_data_P1 %>% select(-`Revised rape rate /2`)
full_data_P1 <- full_data_P1 %>% na.omit()
glimpse(full_data_P1)
filter
```
Before importing the datasets into R, I formatted them in excel to make sure they would already be tidy. Once imported, I chose to do a full join, by "Year", between the two datasets in order to preserve all of the data. A full join combines the two datasets into one comprehensive dataset without deleting any observations from either dataset. This join resulted in no dropped cases, and all of the observations are unique.
```{R}
ezsub <- function(x,y) (x-y)
full_data_P1 <- full_data_P1 %>% mutate(Total_Users = ezsub(Total, `None reported`))
fdp1 <- full_data_P1
fdp12 <- as.tibble(fdp1)
fdp12 <- fdp12 %>% group_by(Year) %>% mutate("NationalPop"=(sum(Population)))
fdp12 <- fdp12 %>% mutate("Fraction_of_National_Pop"=(Population/NationalPop)) %>% mutate("Total_Users_State" = (Fraction_of_National_Pop*`Total_Users`))
```
```{R}
fdp1 %>% summarise_all(funs(n_distinct, first, last))
```
```{R}
fdp1 %>% select(1:31) %>% summarise_all(funs(mean, sd, var, min, max, median, IQR))
```
```{R}
fdp12 %>% filter(`Violent Crime rate`< 1200) %>% arrange(desc(`Violent Crime rate`)) %>% select(`Violent Crime rate`, State, Year)
```
```{R}
fdp1 %>% select(-c(1:31)) %>% group_by(State)%>% summarise_if(is.numeric, funs(mean, var, min, max, sd, median, IQR))
```
```{R}
fdp12 %>% group_by(State, Year) %>% summarise_if(is.numeric,funs(mean, sd))
```
```{R}
select_if(fdp12, is.numeric) %>% cor() %>% glimpse()
```
The first thing I did here is create a function called "ezsub" which just subtracts the values of 2 variables from each other (x,y). I then used this function, in combination with mutate, to create a new varible called "Total_Users" which represents the total number of people who participated in the drug use survey each year less the number of participants who did not report any drug use each year. Mutate was then also used to create 3 more new variables."NationalPop" represents the total US population as reported each year, "Fraction_of_National_Pop" represents how much of the national population each state represented each year, "Total_Users_State" represents the estimated number of users, who participated in the self-reporting survey, in each state, by year. I then calculated the number of distinct obervations, the first observation, and the last observation for each variable in the complete dataset.I then calculated the mean, standard variation, variance, minimum value, maximum value, median, and IQR for all of the variables in the original "drug_use_table" dataset. Next, I created a table which showed the violent crime rates in each state, not including DC since it isn't a state and the crime rates were disproporionately high, for each year, organized in descending order(from greatest to least). The DC data was filtered out of the dataset by filtering out any violent crime rates above 1200. Next, I calculated the mean, standard variation, variance, minimum value, maximum value, median, and IQR for each numeric variable in the original "CrimeStatebyState_2_" dataset, grouped by state. Grouping by state creates a summary variable for each state as opposed to just a general summary variable of total observations in a column. Next, I calculated the mean and standard variation for all of the numeric variables in the full joined dataset, grouped by year and state. However, since the the dataset is tidy and all of the observations are unique, grouping by state and year resulted in no change between the summarized values and the corresponding original values. Lastly, I created a correlation matrix for all of the numeric variables in the full dataset. Values in the correlation matrix range from -1 to 1 with high positive values representing a high correlation and vice versa. 
```{R}
fdp12 %>% pivot_wider(names_from="Year", values_from=`Murder and nonnegligent manslaughter rate`)
```
```{R}

fdp12 %>% pivot_wider(names_from="Year", values_from=`Murder and nonnegligent manslaughter rate`) %>% pivot_longer(c("2004","2005","2006","2007","2008","2009","2010","2011","2012","2013","2014"), names_to="Year", values_to="Murder and nonnegligent manslaughter rate") %>% na.omit()
```
Since my data was already tidy, I first untidied it by creating seperate variables for the Murder and nonnegligent manslaughter rate from each year using pivot_wider(). I then retured the data to its orginal tidy form by condesing those newly created varibles into 2 varaibles, one for year and one for the Murder and nonnegligent manslaughter rate.
```{R}
plot1<-fdp12 %>% group_by(State)%>%mutate(cum_sum_murderrate=cumsum(`Murder and nonnegligent manslaughter rate`))%>% ggplot(aes(x = State,y = cum_sum_murderrate, fill=Year))+ geom_bar(stat = "identity") + theme(text = element_text(size=8), axis.text.x = element_text(angle=90, hjust=1, vjust = 0.25)) + ggtitle("Where Am I Most Likely To Be Murderd?") + ylab("Cumulative Sum of Murder Rates Over Time")

plot1
```
This bar graph shows the cumulative sum of murder rates in each state from 2004-2014. The murder rate for each year in each state can be differentiated based on color with each progressive year being a lighter shade of blue. There do not appear to be any discernable trends just by looking at the data, but it does appear that those who live in Arizona, DC, and Louisiana are, historically, more likely to be murdered than residents of any other states. 
```{R}
fdp12p2 <- fdp12 %>% filter(State == "TX")
logfun<- function(x) log10(x)
plot2<-ggplot(fdp12p2, aes(x = log10(Fraction_of_National_Pop*`Marijuana/hashish`), y = (Fraction_of_National_Pop*`Alcohol`)))+ geom_point(stat="summary", fun.y= "logfun", size=4,aes(color=((`Violent Crime rate`/100000*Population) + (`Property crime rate`/100000*Population)))) + ggtitle("How Weed and Alcohol Affect Crime in Texas(Scaled)") + ylab("Number of Drinkers") + xlab("Number of Weed Smokers") + labs(color = "Total Crimes") + scale_color_gradient(low="yellow", high="red") + scale_y_continuous(breaks = c(4.700, 4.725, 4.750, 4.775, 4.800, 4.8250, 4.8500)) + theme(legend.text = element_text(angle=45, hjust=1), legend.position = "bottom")

plot2
```
This data plot shows the realtionship between marijuana and alcohol usage and violent crime rate in Texas per year. Each data point represents a specific year by nature. The number of alcohol and marijuana users were scaled to log10 values in order to generate better visualization. The number of total crimes is shown by the color of each point with increasing redness relating to an increasing number of total crimes. There appears to be a positive correlation between the number of alcohol users and the number of marijuana users. There also appears to be a weak positive correlation between the total number of crimes comitted and the number of alcohol users, and an even weaker positive correlation between the number of total crimes and the number of marijuana users. The biggest limitations to this plot is that it assumes that the number of users in Texas is consistent with national trends in usage and that the number of people represented in the self reported drug use data is proportional to Texas' proportion of the national population.
```{R}
scale_dat <- scale(drug_use_table)
wss<-vector()
for(i in 1:10){
temp<-drug_use_table%>%dplyr::select(`Marijuana/hashish`,Cocaine,Heroin)%>%kmeans(.,i)
wss[i]<-temp$tot.withinss
}
ggplot()+geom_point(aes(x=1:10,y=wss))+geom_path(aes(x=1:10,y=wss))+
 xlab("clusters")+scale_x_continuous(breaks=1:10)
```
```{R}
clust_dat <- drug_use_table %>% select(`Marijuana/hashish`,Cocaine,Heroin)
kmeans1<-clust_dat %>%scale%>%kmeans(3)
kmeans1
kmeansclust<-clust_dat%>%mutate(cluster=as.factor(kmeans1$cluster))
kmeansclust%>%ggplot(aes(`Marijuana/hashish`,Cocaine, size=Heroin,shape=cluster))+geom_point()
```
The first thing I did was scale the numeric variables I planned on using in order to make the clustering analysis more accurate. Then I  plotted the within-cluster Sum of Squares(WSS) against the number of clusters and then looked for the "elbow" of the graph in order to determine the number of clusters I should use in my analysis; I settled on 3 clusters. I then used kmeans() to as a means of cluster analysis between marijuana, cocaine, and heroin users in the US from the original "drug_use_table" dataset. I then generated a plot of the clusters in order to visulaize the kmeans() cluster analysis data. My hit rate on the cluster data was pretty high at 84.2%. Now to keep it a buck I'm still not totally sure how cluster analysis is useful, and I'm pretty delerious from sleep deprivation, but here goes my interpretation of the data. There seems to be a weak positive correlation between marijuana and cocaine usage as well as a strong negative correlation between heroin and cocine usage, which makes sense since cocaine and heroin basically have opposite effects.